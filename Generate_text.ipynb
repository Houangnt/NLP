{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "17021256_NguyenTrongHoang.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_wbOlZsjXjs"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.autograd as ag\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98gO9D29jd8G"
      },
      "source": [
        "\n",
        "class VanilaRNNLM(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hiddens, n_outputs, vocab, sigma='sigmoid', phi='softmax'):\n",
        "        \"\"\"\n",
        "        Construct a vanila RNN. \n",
        "        \n",
        "        Params:\n",
        "        n_inputs: number of input neurons, embedding_dim\n",
        "        n_hiddens: number of hidden neurons\n",
        "        n_outputs: number of output neurons,\n",
        "        vocab: a dictionary of the form {word: word_id}\n",
        "        sigma: activation function for hidden layer\n",
        "        phi: output function\n",
        "        \"\"\"\n",
        "        super(VanilaRNNLM, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hiddens = n_hiddens\n",
        "        self.embedding   = torch.nn.Embedding(num_embeddings=len(vocab), embedding_dim=256)\n",
        "        \n",
        "       \n",
        "        self.in2hidden = nn.Linear(n_inputs + n_hiddens, n_hiddens)\n",
        "        self.in2output = nn.Linear(n_inputs + n_hiddens, n_outputs)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "      \n",
        "    \n",
        "    def forward(self, xs, h0):\n",
        "        \"\"\"\n",
        "        Params:\n",
        "        xs: the input sequence [start = 0, x_1, x_2, ..., x_n, end = V]. x_i is the id of the i-th word in the sequence. \n",
        "            For example, xs = [1, 3, 11, 6, 8, 2]\n",
        "        h0: the initial hidden state\n",
        "        \n",
        "        Returns: (ys, hs) where\n",
        "        ys = [y_1, y_2, ..., y_n] and\n",
        "        hs = [h_1, h_2, ..., h_n]\n",
        "        \"\"\"\n",
        "\n",
        "        combined = torch.cat((torch.LongTensor(xs), h0), 1)\n",
        "        hs       = torch.sigmoid(self.in2hidden(combined))\n",
        "        output   = self.in2output(combined)\n",
        "        output   = self.softmax(output)\n",
        "        return output, hs\n",
        "    def init_hidden(self):\n",
        "        hidden = nn.init.kaiming_uniform_(torch.empty(1, self.n_hiddens))\n",
        "        return hidden\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5YvmmsjrkFO"
      },
      "source": [
        "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGFk9pjZjglk"
      },
      "source": [
        "class FancyRNNLM(nn.Module):\n",
        "    def __init__(self, n_inputs, n_hiddens, n_outputs, vocab, sigma='sigmoid', phi='softmax'):\n",
        "        \"\"\"\n",
        "        Construct a fancy RNN, this could be LSTM, GRU, or your own invention.\n",
        "        \n",
        "        Params:\n",
        "        n_inputs: number of input neurons\n",
        "        n_hiddens: number of hidden neurons\n",
        "        n_outputs: number of output neurons\n",
        "        vocab: a dictionary {word: word_id}\n",
        "        sigma: activation function for hidden layer\n",
        "        phi: output function\n",
        "        \"\"\"\n",
        "        super(FancyRNNLM, self).__init__()\n",
        "        self.n_inputs = n_inputs\n",
        "        self.n_hiddens = n_hiddens\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding   = torch.nn.Embedding(num_embeddings=len(vocab), embedding_dim=256)\n",
        "       \n",
        "\n",
        "\n",
        "        self.lstm   = nn.LSTM(n_inputs, n_hiddens, 2, batch_first=True, dropout=0.2)\n",
        "        self.fc       = nn.Linear(n_hiddens, n_outputs)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, xs, h0):\n",
        "      embeds = self.embedding(torch.LongTensor(xs))\n",
        "      lstm_out, hs = self.lstm(embeds, h0)\n",
        "      lstm_out = lstm_out.contiguous().view(-1, self.n_hiddens)\n",
        "      out = nn.Dropout(lstm_out)\n",
        "      out = self.fc(out)\n",
        "      out = torch.sigmoid(out)\n",
        "      out = out.view(batch_size, -1)\n",
        "      out = out[:,-1]\n",
        "      return output, hs\n",
        "    def init_hidden(self):\n",
        "      hidden = torch.zeros(self.n_layers, 1, self.n_hiddens).to(device)\n",
        "      return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzAj9t-gjjrT",
        "outputId": "b104045d-5018-4d69-904f-d53fd50b7651"
      },
      "source": [
        "!git clone https://github.com/NTHoang99/NLP.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP'...\n",
            "remote: Enumerating objects: 19, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 19 (delta 4), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (19/19), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Gpzq8pNtk7fF",
        "outputId": "af72400b-5fc3-4f64-d548-530fbd5aaea3"
      },
      "source": [
        "!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n",
            "Collecting torch_nightly\n",
            "  Downloading https://download.pytorch.org/whl/nightly/cu92/torch_nightly-1.2.0.dev20190805%2Bcu92-cp37-cp37m-linux_x86_64.whl (704.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 704.8 MB 5.1 kB/s \n",
            "\u001b[?25hInstalling collected packages: torch-nightly\n",
            "Successfully installed torch-nightly-1.2.0.dev20190805+cu92\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhKiBU2qlHJD",
        "outputId": "18be0071-f846-4afe-9407-863567101721"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 19 12:49:24 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBqVmVYjlYTJ",
        "outputId": "8bf8cdcb-a3a6-4025-d5d5-6ffed4598f78"
      },
      "source": [
        "%cd /content/NLP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuBlNspqphmT",
        "outputId": "45b3fe2b-3619-44be-e6ad-fb13cb40d992"
      },
      "source": [
        "!gdown --id 148kFotM7dTaBfyD_HMkA5BSW-NFXb5Hu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=148kFotM7dTaBfyD_HMkA5BSW-NFXb5Hu\n",
            "To: /content/NLP/wikitext-103.zip\n",
            "100% 189M/189M [00:01<00:00, 101MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMGVyde_plLH",
        "outputId": "61896bc9-df79-49b3-befe-a783da110957"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2zqF9cJpu-Z"
      },
      "source": [
        "!unzip -uq \"/content/NLP/wikitext-103.zip\" -d \"/content/NLP/data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pwp4_uuFlJ-F",
        "outputId": "f44dce7f-82fb-462e-c549-8544ade67089"
      },
      "source": [
        "%%time \n",
        "!python -u main.py --cuda --emsize 650 --nhid 650 --dropout 0.2 --epochs 40 --tied  2>&1 | tee train.log  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:\n",
            "Train:  102590700\n",
            "Valid:  216347\n",
            "Test:   244102\n",
            "| epoch   1 |   200/146558 batches | lr 20.00 | ms/batch 151.98 | loss  9.19 | ppl  9821.10\n",
            "| epoch   1 |   400/146558 batches | lr 20.00 | ms/batch 151.33 | loss  7.83 | ppl  2523.87\n",
            "| epoch   1 |   600/146558 batches | lr 20.00 | ms/batch 151.65 | loss  7.33 | ppl  1532.41\n",
            "| epoch   1 |   800/146558 batches | lr 20.00 | ms/batch 151.68 | loss  7.07 | ppl  1178.19\n",
            "| epoch   1 |  1000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.86 | ppl   950.11\n",
            "| epoch   1 |  1200/146558 batches | lr 20.00 | ms/batch 151.71 | loss  6.73 | ppl   837.97\n",
            "| epoch   1 |  1400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.73 | ppl   833.02\n",
            "| epoch   1 |  1600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.60 | ppl   733.78\n",
            "| epoch   1 |  1800/146558 batches | lr 20.00 | ms/batch 151.71 | loss  6.52 | ppl   676.96\n",
            "| epoch   1 |  2000/146558 batches | lr 20.00 | ms/batch 151.70 | loss  6.39 | ppl   595.74\n",
            "| epoch   1 |  2200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  6.33 | ppl   559.12\n",
            "| epoch   1 |  2400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.24 | ppl   513.69\n",
            "| epoch   1 |  2600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.23 | ppl   505.95\n",
            "| epoch   1 |  2800/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.20 | ppl   494.78\n",
            "| epoch   1 |  3000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.13 | ppl   460.90\n",
            "| epoch   1 |  3200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.07 | ppl   434.54\n",
            "| epoch   1 |  3400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  6.08 | ppl   438.76\n",
            "| epoch   1 |  3600/146558 batches | lr 20.00 | ms/batch 151.70 | loss  6.08 | ppl   437.42\n",
            "| epoch   1 |  3800/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.90 | ppl   365.69\n",
            "| epoch   1 |  4000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.90 | ppl   365.48\n",
            "| epoch   1 |  4200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.93 | ppl   376.12\n",
            "| epoch   1 |  4400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.95 | ppl   383.09\n",
            "| epoch   1 |  4600/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.92 | ppl   372.42\n",
            "| epoch   1 |  4800/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.91 | ppl   368.25\n",
            "| epoch   1 |  5000/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.89 | ppl   361.88\n",
            "| epoch   1 |  5200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.79 | ppl   328.18\n",
            "| epoch   1 |  5400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.76 | ppl   318.20\n",
            "| epoch   1 |  5600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.75 | ppl   312.85\n",
            "| epoch   1 |  5800/146558 batches | lr 20.00 | ms/batch 151.68 | loss  5.70 | ppl   300.09\n",
            "| epoch   1 |  6000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.81 | ppl   333.41\n",
            "| epoch   1 |  6200/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.73 | ppl   308.08\n",
            "| epoch   1 |  6400/146558 batches | lr 20.00 | ms/batch 151.68 | loss  5.76 | ppl   318.03\n",
            "| epoch   1 |  6600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.72 | ppl   306.37\n",
            "| epoch   1 |  6800/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.68 | ppl   291.59\n",
            "| epoch   1 |  7000/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.74 | ppl   312.00\n",
            "| epoch   1 |  7200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.67 | ppl   290.62\n",
            "| epoch   1 |  7400/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.70 | ppl   298.74\n",
            "| epoch   1 |  7600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.71 | ppl   303.21\n",
            "| epoch   1 |  7800/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.60 | ppl   271.69\n",
            "| epoch   1 |  8000/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.61 | ppl   274.09\n",
            "| epoch   1 |  8200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.69 | ppl   297.12\n",
            "| epoch   1 |  8400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.68 | ppl   292.43\n",
            "| epoch   1 |  8600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.75 | ppl   312.76\n",
            "| epoch   1 |  8800/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.61 | ppl   271.80\n",
            "| epoch   1 |  9000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.63 | ppl   278.72\n",
            "| epoch   1 |  9200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.64 | ppl   281.08\n",
            "| epoch   1 |  9400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.62 | ppl   276.80\n",
            "| epoch   1 |  9600/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.60 | ppl   271.16\n",
            "| epoch   1 |  9800/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.58 | ppl   264.53\n",
            "| epoch   1 | 10000/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.51 | ppl   247.86\n",
            "| epoch   1 | 10200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.53 | ppl   252.11\n",
            "| epoch   1 | 10400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.63 | ppl   279.07\n",
            "| epoch   1 | 10600/146558 batches | lr 20.00 | ms/batch 151.72 | loss  5.57 | ppl   261.77\n",
            "| epoch   1 | 10800/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.43 | ppl   227.07\n",
            "| epoch   1 | 11000/146558 batches | lr 20.00 | ms/batch 151.72 | loss  5.48 | ppl   240.62\n",
            "| epoch   1 | 11200/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.56 | ppl   260.22\n",
            "| epoch   1 | 11400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.52 | ppl   250.15\n",
            "| epoch   1 | 11600/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.48 | ppl   238.89\n",
            "| epoch   1 | 11800/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.52 | ppl   249.33\n",
            "| epoch   1 | 12000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.51 | ppl   248.39\n",
            "| epoch   1 | 12200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.49 | ppl   243.34\n",
            "| epoch   1 | 12400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.54 | ppl   254.56\n",
            "| epoch   1 | 12600/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.55 | ppl   257.44\n",
            "| epoch   1 | 12800/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.44 | ppl   230.36\n",
            "| epoch   1 | 13000/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.47 | ppl   238.59\n",
            "| epoch   1 | 13200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.48 | ppl   239.34\n",
            "| epoch   1 | 13400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.51 | ppl   246.44\n",
            "| epoch   1 | 13600/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.43 | ppl   228.77\n",
            "| epoch   1 | 13800/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.39 | ppl   218.48\n",
            "| epoch   1 | 14000/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.50 | ppl   243.87\n",
            "| epoch   1 | 14200/146558 batches | lr 20.00 | ms/batch 151.72 | loss  5.39 | ppl   219.12\n",
            "| epoch   1 | 14400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.45 | ppl   231.82\n",
            "| epoch   1 | 14600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.35 | ppl   210.35\n",
            "| epoch   1 | 14800/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.35 | ppl   209.97\n",
            "| epoch   1 | 15000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.42 | ppl   225.43\n",
            "| epoch   1 | 15200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.42 | ppl   225.83\n",
            "| epoch   1 | 15400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.43 | ppl   227.38\n",
            "| epoch   1 | 15600/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.45 | ppl   232.62\n",
            "| epoch   1 | 15800/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.46 | ppl   235.16\n",
            "| epoch   1 | 16000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.45 | ppl   232.39\n",
            "| epoch   1 | 16200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.41 | ppl   223.11\n",
            "| epoch   1 | 16400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.38 | ppl   216.15\n",
            "| epoch   1 | 16600/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.41 | ppl   223.78\n",
            "| epoch   1 | 16800/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.40 | ppl   222.04\n",
            "| epoch   1 | 17000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.32 | ppl   205.33\n",
            "| epoch   1 | 17200/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.35 | ppl   210.69\n",
            "| epoch   1 | 17400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.36 | ppl   211.67\n",
            "| epoch   1 | 17600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.42 | ppl   226.85\n",
            "| epoch   1 | 17800/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.41 | ppl   222.97\n",
            "| epoch   1 | 18000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.38 | ppl   216.89\n",
            "| epoch   1 | 18200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.34 | ppl   208.65\n",
            "| epoch   1 | 18400/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.37 | ppl   215.23\n",
            "| epoch   1 | 18600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.34 | ppl   208.23\n",
            "| epoch   1 | 18800/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.30 | ppl   200.28\n",
            "| epoch   1 | 19000/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.33 | ppl   206.35\n",
            "| epoch   1 | 19200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.45 | ppl   233.03\n",
            "| epoch   1 | 19400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.35 | ppl   210.52\n",
            "| epoch   1 | 19600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.20 | ppl   181.91\n",
            "| epoch   1 | 19800/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.29 | ppl   197.79\n",
            "| epoch   1 | 20000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.41 | ppl   224.23\n",
            "| epoch   1 | 20200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.38 | ppl   216.45\n",
            "| epoch   1 | 20400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.29 | ppl   198.16\n",
            "| epoch   1 | 20600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.25 | ppl   191.29\n",
            "| epoch   1 | 20800/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.33 | ppl   205.82\n",
            "| epoch   1 | 21000/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.31 | ppl   203.26\n",
            "| epoch   1 | 21200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.32 | ppl   204.58\n",
            "| epoch   1 | 21400/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.25 | ppl   189.74\n",
            "| epoch   1 | 21600/146558 batches | lr 20.00 | ms/batch 151.68 | loss  5.28 | ppl   197.10\n",
            "| epoch   1 | 21800/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.37 | ppl   215.60\n",
            "| epoch   1 | 22000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.37 | ppl   214.19\n",
            "| epoch   1 | 22200/146558 batches | lr 20.00 | ms/batch 151.72 | loss  5.22 | ppl   185.50\n",
            "| epoch   1 | 22400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.20 | ppl   181.54\n",
            "| epoch   1 | 22600/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.32 | ppl   204.23\n",
            "| epoch   1 | 22800/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.28 | ppl   196.77\n",
            "| epoch   1 | 23000/146558 batches | lr 20.00 | ms/batch 151.71 | loss  5.21 | ppl   183.01\n",
            "| epoch   1 | 23200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.24 | ppl   188.58\n",
            "| epoch   1 | 23400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.31 | ppl   202.64\n",
            "| epoch   1 | 23600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.40 | ppl   221.09\n",
            "CPU times: user 16.6 s, sys: 2.57 s, total: 19.2 s\n",
            "Wall time: 1h 9min 42s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExrHsrPG59pK",
        "outputId": "675e5c32-58cd-4434-e337-35d6a3ba7b49"
      },
      "source": [
        "%%time \n",
        "!python -u main.py --cuda --emsize 650 --nhid 650 --dropout 0.5 --epochs 20 --tied  2>&1 | tee train.log  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:\n",
            "Train:  102590700\n",
            "Valid:  216347\n",
            "Test:   244102\n",
            "| epoch   1 |  1200/146558 batches | lr 20.00 | ms/batch 151.74 | loss  7.50 | ppl  1814.00\n",
            "| epoch   1 |  2400/146558 batches | lr 20.00 | ms/batch 151.75 | loss  6.47 | ppl   643.19\n",
            "| epoch   1 |  3600/146558 batches | lr 20.00 | ms/batch 151.75 | loss  6.13 | ppl   461.19\n",
            "| epoch   1 |  4800/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.92 | ppl   371.79\n",
            "| epoch   1 |  6000/146558 batches | lr 20.00 | ms/batch 151.75 | loss  5.78 | ppl   325.21\n",
            "| epoch   1 |  7200/146558 batches | lr 20.00 | ms/batch 151.75 | loss  5.72 | ppl   304.28\n",
            "| epoch   1 |  8400/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.67 | ppl   289.28\n",
            "| epoch   1 |  9600/146558 batches | lr 20.00 | ms/batch 151.75 | loss  5.64 | ppl   281.71\n",
            "| epoch   1 | 10800/146558 batches | lr 20.00 | ms/batch 151.75 | loss  5.54 | ppl   254.88\n",
            "| epoch   1 | 12000/146558 batches | lr 20.00 | ms/batch 151.75 | loss  5.51 | ppl   247.83\n",
            "| epoch   1 | 13200/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.50 | ppl   243.76\n",
            "| epoch   1 | 14400/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.44 | ppl   231.16\n",
            "| epoch   1 | 15600/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.40 | ppl   221.76\n",
            "| epoch   1 | 16800/146558 batches | lr 20.00 | ms/batch 151.77 | loss  5.42 | ppl   225.34\n",
            "| epoch   1 | 18000/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.37 | ppl   215.61\n",
            "| epoch   1 | 19200/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.36 | ppl   211.72\n",
            "| epoch   1 | 20400/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.32 | ppl   204.36\n",
            "| epoch   1 | 21600/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.29 | ppl   198.53\n",
            "| epoch   1 | 22800/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.29 | ppl   199.21\n",
            "| epoch   1 | 24000/146558 batches | lr 20.00 | ms/batch 151.76 | loss  5.28 | ppl   195.62\n",
            "| epoch   1 | 25200/146558 batches | lr 20.00 | ms/batch 151.73 | loss  5.28 | ppl   196.02\n",
            "| epoch   1 | 26400/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.28 | ppl   196.18\n",
            "| epoch   1 | 27600/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.28 | ppl   195.62\n",
            "| epoch   1 | 28800/146558 batches | lr 20.00 | ms/batch 151.68 | loss  5.19 | ppl   180.02\n",
            "| epoch   1 | 30000/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.23 | ppl   186.58\n",
            "| epoch   1 | 31200/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.19 | ppl   180.09\n",
            "| epoch   1 | 32400/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.21 | ppl   183.16\n",
            "| epoch   1 | 33600/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.21 | ppl   183.89\n",
            "| epoch   1 | 34800/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.19 | ppl   179.70\n",
            "| epoch   1 | 36000/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.23 | ppl   185.97\n",
            "| epoch   1 | 37200/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.18 | ppl   177.00\n",
            "| epoch   1 | 38400/146558 batches | lr 20.00 | ms/batch 151.68 | loss  5.17 | ppl   176.54\n",
            "| epoch   1 | 39600/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.15 | ppl   173.04\n",
            "| epoch   1 | 40800/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.22 | ppl   185.20\n",
            "| epoch   1 | 42000/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.13 | ppl   169.24\n",
            "| epoch   1 | 43200/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.17 | ppl   176.50\n",
            "| epoch   1 | 44400/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.15 | ppl   172.41\n",
            "| epoch   1 | 45600/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.16 | ppl   174.07\n",
            "| epoch   1 | 46800/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.14 | ppl   171.37\n",
            "| epoch   1 | 48000/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.14 | ppl   171.00\n",
            "| epoch   1 | 49200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.14 | ppl   170.40\n",
            "| epoch   1 | 50400/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.14 | ppl   170.81\n",
            "| epoch   1 | 51600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.13 | ppl   168.64\n",
            "| epoch   1 | 52800/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.14 | ppl   169.89\n",
            "| epoch   1 | 54000/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.15 | ppl   172.59\n",
            "| epoch   1 | 55200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.15 | ppl   172.98\n",
            "| epoch   1 | 56400/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.11 | ppl   166.45\n",
            "| epoch   1 | 57600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.12 | ppl   166.54\n",
            "| epoch   1 | 58800/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.13 | ppl   168.97\n",
            "| epoch   1 | 60000/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.12 | ppl   167.06\n",
            "| epoch   1 | 61200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.08 | ppl   161.10\n",
            "| epoch   1 | 62400/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.11 | ppl   164.97\n",
            "| epoch   1 | 63600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.13 | ppl   169.51\n",
            "| epoch   1 | 64800/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.09 | ppl   162.34\n",
            "| epoch   1 | 66000/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.13 | ppl   168.84\n",
            "| epoch   1 | 67200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.10 | ppl   163.64\n",
            "| epoch   1 | 68400/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.13 | ppl   168.27\n",
            "| epoch   1 | 69600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.05 | ppl   155.95\n",
            "| epoch   1 | 70800/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.12 | ppl   167.57\n",
            "| epoch   1 | 72000/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.06 | ppl   157.56\n",
            "| epoch   1 | 73200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.10 | ppl   164.65\n",
            "| epoch   1 | 74400/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.06 | ppl   157.19\n",
            "| epoch   1 | 75600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.08 | ppl   161.14\n",
            "| epoch   1 | 76800/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.07 | ppl   159.84\n",
            "| epoch   1 | 78000/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.07 | ppl   159.38\n",
            "| epoch   1 | 79200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.04 | ppl   153.93\n",
            "| epoch   1 | 80400/146558 batches | lr 20.00 | ms/batch 151.66 | loss  4.98 | ppl   145.13\n",
            "| epoch   1 | 81600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.01 | ppl   150.22\n",
            "| epoch   1 | 82800/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.05 | ppl   156.70\n",
            "| epoch   1 | 84000/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.07 | ppl   158.98\n",
            "| epoch   1 | 85200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.05 | ppl   156.53\n",
            "| epoch   1 | 86400/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.07 | ppl   159.94\n",
            "| epoch   1 | 87600/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.02 | ppl   151.60\n",
            "| epoch   1 | 88800/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.01 | ppl   150.07\n",
            "| epoch   1 | 90000/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.06 | ppl   157.31\n",
            "| epoch   1 | 91200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.04 | ppl   154.28\n",
            "| epoch   1 | 92400/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.04 | ppl   154.80\n",
            "| epoch   1 | 93600/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.05 | ppl   155.29\n",
            "| epoch   1 | 94800/146558 batches | lr 20.00 | ms/batch 151.68 | loss  5.04 | ppl   155.18\n",
            "| epoch   1 | 96000/146558 batches | lr 20.00 | ms/batch 151.68 | loss  5.05 | ppl   155.35\n",
            "| epoch   1 | 97200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.09 | ppl   162.54\n",
            "| epoch   1 | 98400/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.05 | ppl   156.57\n",
            "| epoch   1 | 99600/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.04 | ppl   154.61\n",
            "| epoch   1 | 100800/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.02 | ppl   152.15\n",
            "| epoch   1 | 102000/146558 batches | lr 20.00 | ms/batch 151.69 | loss  5.02 | ppl   151.78\n",
            "| epoch   1 | 103200/146558 batches | lr 20.00 | ms/batch 151.70 | loss  5.01 | ppl   150.65\n",
            "| epoch   1 | 104400/146558 batches | lr 20.00 | ms/batch 151.67 | loss  5.05 | ppl   155.84\n",
            "| epoch   1 | 105600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.02 | ppl   150.88\n",
            "| epoch   1 | 106800/146558 batches | lr 20.00 | ms/batch 151.65 | loss  5.06 | ppl   157.37\n",
            "| epoch   1 | 108000/146558 batches | lr 20.00 | ms/batch 151.65 | loss  5.02 | ppl   152.12\n",
            "| epoch   1 | 109200/146558 batches | lr 20.00 | ms/batch 151.65 | loss  5.03 | ppl   153.37\n",
            "| epoch   1 | 110400/146558 batches | lr 20.00 | ms/batch 151.65 | loss  5.00 | ppl   147.84\n",
            "| epoch   1 | 111600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  4.99 | ppl   146.43\n",
            "| epoch   1 | 112800/146558 batches | lr 20.00 | ms/batch 151.66 | loss  4.97 | ppl   143.95\n",
            "| epoch   1 | 114000/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.04 | ppl   154.59\n",
            "| epoch   1 | 115200/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.01 | ppl   149.78\n",
            "| epoch   1 | 116400/146558 batches | lr 20.00 | ms/batch 151.66 | loss  4.98 | ppl   145.30\n",
            "| epoch   1 | 117600/146558 batches | lr 20.00 | ms/batch 151.66 | loss  5.02 | ppl   151.66\n",
            "| epoch   1 | 118800/146558 batches | lr 20.00 | ms/batch 151.64 | loss  5.05 | ppl   156.58\n",
            "| epoch   1 | 120000/146558 batches | lr 20.00 | ms/batch 151.65 | loss  5.00 | ppl   149.02\n",
            "| epoch   1 | 121200/146558 batches | lr 20.00 | ms/batch 151.64 | loss  4.99 | ppl   146.47\n",
            "| epoch   1 | 122400/146558 batches | lr 20.00 | ms/batch 151.64 | loss  5.01 | ppl   149.90\n",
            "| epoch   1 | 123600/146558 batches | lr 20.00 | ms/batch 151.65 | loss  4.97 | ppl   144.18\n",
            "| epoch   1 | 124800/146558 batches | lr 20.00 | ms/batch 151.64 | loss  4.97 | ppl   143.98\n",
            "| epoch   1 | 126000/146558 batches | lr 20.00 | ms/batch 151.62 | loss  4.99 | ppl   147.31\n",
            "| epoch   1 | 127200/146558 batches | lr 20.00 | ms/batch 151.61 | loss  5.01 | ppl   149.33\n",
            "| epoch   1 | 128400/146558 batches | lr 20.00 | ms/batch 151.60 | loss  4.98 | ppl   145.45\n",
            "| epoch   1 | 129600/146558 batches | lr 20.00 | ms/batch 151.63 | loss  5.00 | ppl   147.85\n",
            "| epoch   1 | 130800/146558 batches | lr 20.00 | ms/batch 151.62 | loss  5.02 | ppl   151.93\n",
            "| epoch   1 | 132000/146558 batches | lr 20.00 | ms/batch 151.62 | loss  4.98 | ppl   145.87\n",
            "| epoch   1 | 133200/146558 batches | lr 20.00 | ms/batch 151.62 | loss  4.94 | ppl   139.58\n",
            "| epoch   1 | 134400/146558 batches | lr 20.00 | ms/batch 151.60 | loss  4.98 | ppl   146.10\n",
            "| epoch   1 | 135600/146558 batches | lr 20.00 | ms/batch 151.59 | loss  5.02 | ppl   151.13\n",
            "| epoch   1 | 136800/146558 batches | lr 20.00 | ms/batch 151.53 | loss  5.03 | ppl   153.00\n",
            "| epoch   1 | 138000/146558 batches | lr 20.00 | ms/batch 151.58 | loss  4.98 | ppl   146.11\n",
            "| epoch   1 | 139200/146558 batches | lr 20.00 | ms/batch 151.60 | loss  5.01 | ppl   149.91\n",
            "| epoch   1 | 140400/146558 batches | lr 20.00 | ms/batch 151.59 | loss  4.98 | ppl   145.76\n",
            "| epoch   1 | 141600/146558 batches | lr 20.00 | ms/batch 151.56 | loss  4.96 | ppl   143.13\n",
            "| epoch   1 | 142800/146558 batches | lr 20.00 | ms/batch 151.49 | loss  4.94 | ppl   139.11\n",
            "| epoch   1 | 144000/146558 batches | lr 20.00 | ms/batch 151.54 | loss  4.99 | ppl   146.36\n",
            "| epoch   1 | 145200/146558 batches | lr 20.00 | ms/batch 151.48 | loss  5.00 | ppl   147.83\n",
            "| epoch   1 | 146400/146558 batches | lr 20.00 | ms/batch 151.53 | loss  4.95 | ppl   141.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 22242.59s | valid loss  4.69 | valid ppl   109.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |  1200/146558 batches | lr 20.00 | ms/batch 151.69 | loss  4.94 | ppl   140.29\n",
            "| epoch   2 |  2400/146558 batches | lr 20.00 | ms/batch 151.63 | loss  4.98 | ppl   145.98\n",
            "| epoch   2 |  3600/146558 batches | lr 20.00 | ms/batch 151.59 | loss  4.98 | ppl   145.78\n",
            "| epoch   2 |  4800/146558 batches | lr 20.00 | ms/batch 151.59 | loss  4.96 | ppl   142.22\n",
            "| epoch   2 |  6000/146558 batches | lr 20.00 | ms/batch 151.53 | loss  4.94 | ppl   139.18\n",
            "| epoch   2 |  7200/146558 batches | lr 20.00 | ms/batch 151.59 | loss  4.95 | ppl   141.44\n",
            "| epoch   2 |  8400/146558 batches | lr 20.00 | ms/batch 151.62 | loss  4.97 | ppl   144.67\n",
            "| epoch   2 |  9600/146558 batches | lr 20.00 | ms/batch 151.53 | loss  5.01 | ppl   150.23\n",
            "| epoch   2 | 10800/146558 batches | lr 20.00 | ms/batch 151.57 | loss  4.95 | ppl   141.82\n",
            "| epoch   2 | 12000/146558 batches | lr 20.00 | ms/batch 151.50 | loss  4.96 | ppl   143.16\n",
            "| epoch   2 | 13200/146558 batches | lr 20.00 | ms/batch 151.52 | loss  4.97 | ppl   143.92\n",
            "| epoch   2 | 14400/146558 batches | lr 20.00 | ms/batch 151.48 | loss  4.96 | ppl   142.91\n",
            "| epoch   2 | 15600/146558 batches | lr 20.00 | ms/batch 151.49 | loss  4.94 | ppl   139.57\n",
            "| epoch   2 | 16800/146558 batches | lr 20.00 | ms/batch 151.53 | loss  4.98 | ppl   144.80\n",
            "| epoch   2 | 18000/146558 batches | lr 20.00 | ms/batch 151.48 | loss  4.95 | ppl   141.16\n",
            "| epoch   2 | 19200/146558 batches | lr 20.00 | ms/batch 151.46 | loss  4.94 | ppl   139.82\n",
            "| epoch   2 | 20400/146558 batches | lr 20.00 | ms/batch 151.44 | loss  4.92 | ppl   137.56\n",
            "| epoch   2 | 21600/146558 batches | lr 20.00 | ms/batch 151.53 | loss  4.91 | ppl   135.10\n",
            "| epoch   2 | 22800/146558 batches | lr 20.00 | ms/batch 151.48 | loss  4.93 | ppl   138.11\n",
            "| epoch   2 | 24000/146558 batches | lr 20.00 | ms/batch 151.36 | loss  4.93 | ppl   137.88\n",
            "| epoch   2 | 25200/146558 batches | lr 20.00 | ms/batch 151.41 | loss  4.93 | ppl   138.57\n",
            "| epoch   2 | 26400/146558 batches | lr 20.00 | ms/batch 151.43 | loss  4.95 | ppl   140.97\n",
            "| epoch   2 | 27600/146558 batches | lr 20.00 | ms/batch 151.29 | loss  4.95 | ppl   141.74\n",
            "| epoch   2 | 28800/146558 batches | lr 20.00 | ms/batch 151.51 | loss  4.87 | ppl   130.65\n",
            "| epoch   2 | 30000/146558 batches | lr 20.00 | ms/batch 151.43 | loss  4.92 | ppl   137.21\n",
            "| epoch   2 | 31200/146558 batches | lr 20.00 | ms/batch 151.41 | loss  4.89 | ppl   133.33\n",
            "| epoch   2 | 32400/146558 batches | lr 20.00 | ms/batch 151.40 | loss  4.91 | ppl   136.16\n",
            "| epoch   2 | 33600/146558 batches | lr 20.00 | ms/batch 151.29 | loss  4.93 | ppl   138.47\n",
            "| epoch   2 | 34800/146558 batches | lr 20.00 | ms/batch 151.29 | loss  4.91 | ppl   136.03\n",
            "| epoch   2 | 36000/146558 batches | lr 20.00 | ms/batch 151.22 | loss  4.95 | ppl   140.87\n",
            "| epoch   2 | 37200/146558 batches | lr 20.00 | ms/batch 151.34 | loss  4.90 | ppl   134.21\n",
            "| epoch   2 | 38400/146558 batches | lr 20.00 | ms/batch 151.38 | loss  4.91 | ppl   135.12\n",
            "| epoch   2 | 39600/146558 batches | lr 20.00 | ms/batch 151.37 | loss  4.90 | ppl   133.76\n",
            "| epoch   2 | 40800/146558 batches | lr 20.00 | ms/batch 151.05 | loss  4.98 | ppl   144.82\n",
            "| epoch   2 | 42000/146558 batches | lr 20.00 | ms/batch 151.17 | loss  4.88 | ppl   131.86\n",
            "| epoch   2 | 43200/146558 batches | lr 20.00 | ms/batch 151.22 | loss  4.93 | ppl   138.93\n",
            "| epoch   2 | 44400/146558 batches | lr 20.00 | ms/batch 151.12 | loss  4.92 | ppl   136.60\n",
            "| epoch   2 | 45600/146558 batches | lr 20.00 | ms/batch 151.22 | loss  4.93 | ppl   138.32\n",
            "| epoch   2 | 46800/146558 batches | lr 20.00 | ms/batch 151.17 | loss  4.91 | ppl   135.34\n",
            "| epoch   2 | 48000/146558 batches | lr 20.00 | ms/batch 151.00 | loss  4.92 | ppl   136.79\n",
            "| epoch   2 | 49200/146558 batches | lr 20.00 | ms/batch 151.12 | loss  4.92 | ppl   136.37\n",
            "| epoch   2 | 50400/146558 batches | lr 20.00 | ms/batch 151.12 | loss  4.92 | ppl   137.02\n",
            "| epoch   2 | 51600/146558 batches | lr 20.00 | ms/batch 151.08 | loss  4.91 | ppl   135.13\n",
            "| epoch   2 | 52800/146558 batches | lr 20.00 | ms/batch 151.12 | loss  4.92 | ppl   136.86\n",
            "| epoch   2 | 54000/146558 batches | lr 20.00 | ms/batch 151.15 | loss  4.94 | ppl   139.56\n",
            "| epoch   2 | 55200/146558 batches | lr 20.00 | ms/batch 151.19 | loss  4.94 | ppl   139.90\n",
            "| epoch   2 | 56400/146558 batches | lr 20.00 | ms/batch 151.04 | loss  4.91 | ppl   135.68\n",
            "| epoch   2 | 57600/146558 batches | lr 20.00 | ms/batch 150.99 | loss  4.92 | ppl   136.34\n",
            "| epoch   2 | 58800/146558 batches | lr 20.00 | ms/batch 151.02 | loss  4.93 | ppl   138.28\n",
            "| epoch   2 | 60000/146558 batches | lr 20.00 | ms/batch 151.04 | loss  4.93 | ppl   137.93\n",
            "| epoch   2 | 61200/146558 batches | lr 20.00 | ms/batch 151.10 | loss  4.89 | ppl   132.94\n",
            "| epoch   2 | 62400/146558 batches | lr 20.00 | ms/batch 150.96 | loss  4.91 | ppl   135.48\n",
            "| epoch   2 | 63600/146558 batches | lr 20.00 | ms/batch 150.86 | loss  4.94 | ppl   140.44\n",
            "| epoch   2 | 64800/146558 batches | lr 20.00 | ms/batch 150.83 | loss  4.91 | ppl   135.02\n",
            "| epoch   2 | 66000/146558 batches | lr 20.00 | ms/batch 150.85 | loss  4.95 | ppl   140.85\n",
            "| epoch   2 | 67200/146558 batches | lr 20.00 | ms/batch 151.05 | loss  4.92 | ppl   136.70\n",
            "| epoch   2 | 68400/146558 batches | lr 20.00 | ms/batch 150.62 | loss  4.95 | ppl   141.31\n",
            "| epoch   2 | 69600/146558 batches | lr 20.00 | ms/batch 151.08 | loss  4.86 | ppl   129.53\n",
            "| epoch   2 | 70800/146558 batches | lr 20.00 | ms/batch 150.89 | loss  4.95 | ppl   141.14\n",
            "| epoch   2 | 72000/146558 batches | lr 20.00 | ms/batch 150.99 | loss  4.88 | ppl   132.19\n",
            "| epoch   2 | 73200/146558 batches | lr 20.00 | ms/batch 150.99 | loss  4.92 | ppl   137.65\n",
            "| epoch   2 | 74400/146558 batches | lr 20.00 | ms/batch 151.01 | loss  4.89 | ppl   133.48\n",
            "| epoch   2 | 75600/146558 batches | lr 20.00 | ms/batch 150.72 | loss  4.92 | ppl   137.03\n",
            "| epoch   2 | 76800/146558 batches | lr 20.00 | ms/batch 150.77 | loss  4.91 | ppl   135.34\n",
            "| epoch   2 | 78000/146558 batches | lr 20.00 | ms/batch 150.71 | loss  4.90 | ppl   134.70\n",
            "| epoch   2 | 79200/146558 batches | lr 20.00 | ms/batch 150.77 | loss  4.87 | ppl   130.63\n",
            "| epoch   2 | 80400/146558 batches | lr 20.00 | ms/batch 150.92 | loss  4.82 | ppl   124.00\n",
            "| epoch   2 | 81600/146558 batches | lr 20.00 | ms/batch 150.72 | loss  4.86 | ppl   128.81\n",
            "| epoch   2 | 82800/146558 batches | lr 20.00 | ms/batch 150.73 | loss  4.89 | ppl   133.24\n",
            "| epoch   2 | 84000/146558 batches | lr 20.00 | ms/batch 150.69 | loss  4.91 | ppl   135.96\n",
            "| epoch   2 | 85200/146558 batches | lr 20.00 | ms/batch 150.58 | loss  4.91 | ppl   135.06\n",
            "| epoch   2 | 86400/146558 batches | lr 20.00 | ms/batch 150.65 | loss  4.92 | ppl   137.03\n",
            "| epoch   2 | 87600/146558 batches | lr 20.00 | ms/batch 150.66 | loss  4.87 | ppl   130.54\n",
            "| epoch   2 | 88800/146558 batches | lr 20.00 | ms/batch 150.75 | loss  4.86 | ppl   129.32\n",
            "| epoch   2 | 90000/146558 batches | lr 20.00 | ms/batch 150.68 | loss  4.91 | ppl   135.61\n",
            "| epoch   2 | 91200/146558 batches | lr 20.00 | ms/batch 150.61 | loss  4.89 | ppl   133.19\n",
            "| epoch   2 | 92400/146558 batches | lr 20.00 | ms/batch 150.56 | loss  4.89 | ppl   133.20\n",
            "| epoch   2 | 93600/146558 batches | lr 20.00 | ms/batch 150.43 | loss  4.90 | ppl   134.74\n",
            "| epoch   2 | 94800/146558 batches | lr 20.00 | ms/batch 150.63 | loss  4.90 | ppl   134.47\n",
            "| epoch   2 | 96000/146558 batches | lr 20.00 | ms/batch 150.58 | loss  4.90 | ppl   134.20\n",
            "| epoch   2 | 97200/146558 batches | lr 20.00 | ms/batch 150.53 | loss  4.94 | ppl   140.46\n",
            "| epoch   2 | 98400/146558 batches | lr 20.00 | ms/batch 150.43 | loss  4.91 | ppl   135.92\n",
            "| epoch   2 | 99600/146558 batches | lr 20.00 | ms/batch 150.62 | loss  4.90 | ppl   133.88\n",
            "| epoch   2 | 100800/146558 batches | lr 20.00 | ms/batch 150.45 | loss  4.89 | ppl   132.34\n",
            "| epoch   2 | 102000/146558 batches | lr 20.00 | ms/batch 150.43 | loss  4.88 | ppl   132.17\n",
            "| epoch   2 | 103200/146558 batches | lr 20.00 | ms/batch 150.44 | loss  4.88 | ppl   131.24\n",
            "| epoch   2 | 104400/146558 batches | lr 20.00 | ms/batch 150.51 | loss  4.91 | ppl   135.21\n",
            "| epoch   2 | 105600/146558 batches | lr 20.00 | ms/batch 150.60 | loss  4.88 | ppl   131.83\n",
            "| epoch   2 | 106800/146558 batches | lr 20.00 | ms/batch 150.27 | loss  4.93 | ppl   137.92\n",
            "| epoch   2 | 108000/146558 batches | lr 20.00 | ms/batch 150.42 | loss  4.90 | ppl   133.91\n",
            "| epoch   2 | 109200/146558 batches | lr 20.00 | ms/batch 150.29 | loss  4.90 | ppl   134.95\n",
            "CPU times: user 1min 41s, sys: 16.3 s, total: 1min 57s\n",
            "Wall time: 10h 58min 6s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eAqleaXBU5t"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from model import RNNModel\n",
        "from data import Dictionary, Corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrwQefMJB6eS",
        "outputId": "81e89197-3239-4c17-c61d-db7bc779c049"
      },
      "source": [
        "DATA_PATH = \"/content/NLP/data/wikitext-103\"\n",
        "corpus = Corpus(DATA_PATH)\n",
        "\n",
        "print(\"Number of tokens:\")\n",
        "print(\"Train: \", len(corpus.train))\n",
        "print(\"Valid: \", len(corpus.valid))\n",
        "print(\"Test:  \", len(corpus.test))\n",
        "\n",
        "print(\"Vocabulary size:\", len(corpus.dictionary.idx2word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens:\n",
            "Train:  102590700\n",
            "Valid:  216347\n",
            "Test:   244102\n",
            "Vocabulary size: 267735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPgBLhbK30YV"
      },
      "source": [
        "DEVICE = torch.device(\"cpu\")\n",
        "with open(\"/content/NLP/model.pt\", 'rb') as f:\n",
        "    model = torch.load(f, map_location='cpu')\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGBu06Ln3_6b",
        "outputId": "5645a398-bd92-44b4-b82b-17c507fadd30"
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNNModel(\n",
              "  (drop): Dropout(p=0.2, inplace=False)\n",
              "  (encoder): Embedding(267735, 200)\n",
              "  (rnn): LSTM(200, 200, num_layers=2, dropout=0.2)\n",
              "  (decoder): Linear(in_features=200, out_features=267735, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXwXAQgn4EA1"
      },
      "source": [
        "**Perplexity (+2 bonus points)**\n",
        "\n",
        "Compute the perplexity of the models. The lower the perplexity, the higher your score.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OydgLvu4b-6"
      },
      "source": [
        "%%time\n",
        "BPTT = 50\n",
        "CRITERION = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(DEVICE)\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(BPTT, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(10)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, BPTT):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * CRITERION(output_flat, targets).item()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / len(data_source)\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "    \n",
        "test_data = batchify(corpus.train, 10)\n",
        "loss = evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oqol6vwQ4l8b",
        "outputId": "ae605c99-0bbe-4263-85c0-730c91f80bef"
      },
      "source": [
        "loss, np.exp(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.999740528464561, 148.37465510786626)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaOTmNo_qAiw",
        "outputId": "28546863-fa29-4c92-caa1-d439b1124c92"
      },
      "source": [
        "\n",
        "%%time\n",
        "BPTT = 50\n",
        "CRITERION = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(DEVICE)\n",
        "\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(BPTT, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target\n",
        "\n",
        "def evaluate(data_source):\n",
        "    # Turn on evaluation mode which disables dropout.\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    ntokens = len(corpus.dictionary)\n",
        "    hidden = model.init_hidden(10)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, BPTT):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output, hidden = model(data, hidden)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * CRITERION(output_flat, targets).item()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "    return total_loss / len(data_source)\n",
        "\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)\n",
        "    \n",
        "test_data = batchify(corpus.test, 10)\n",
        "loss = evaluate(test_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 8min 53s, sys: 3.08 s, total: 8min 56s\n",
            "Wall time: 4min 27s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xeuuvl8xsUDa",
        "outputId": "0c6a22d4-c28b-4890-a7c7-0de3f8129297"
      },
      "source": [
        "loss, np.exp(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4.999740528464561, 148.37465510786626)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl50M-9O4o9H"
      },
      "source": [
        "**Generating new text with RNNLM**\n",
        "\n",
        "Write the code to generate new text segments from the RNNLM. Produce several outputs from both VanilaRNN and FancyRNN to compare the quality of 2 models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taakREiN4ocI",
        "outputId": "a1fec7a7-3ba1-4950-cf48-6c3c0e22222e"
      },
      "source": [
        "test_tokens = corpus.test.numpy()\n",
        "eos_pos = np.where(test_tokens == corpus.dictionary.word2idx[\"<eos>\"])[0]\n",
        "print(\"Number of lines in test:\", len(eos_pos))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines in test: 2891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vb31oIUH4yep",
        "outputId": "47f9dcc5-49ce-4b65-c303-fbe7de268ae3"
      },
      "source": [
        "print(\" \".join([corpus.dictionary.idx2word[c] for c in test_tokens[eos_pos[28]+1:eos_pos[29]]]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The An Lushan Rebellion began in December 755 , and was not completely suppressed for almost eight years . It caused enormous disruption to Chinese society : the census of 754 recorded 52 @.@ 9 million people , but ten years later , the census counted just 16 @.@ 9 million , the remainder having been displaced or killed . During this time , Du Fu led a largely itinerant life unsettled by wars , associated famines and imperial displeasure . This period of unhappiness was the making of Du Fu as a poet : Even Shan Chou has written that , \" What he saw around him — the lives of his family , neighbors , and strangers – what he heard , and what he hoped for or feared from the progress of various campaigns — these became the enduring themes of his poetry \" . Even when he learned of the death of his youngest child , he turned to the suffering of others in his poetry instead of dwelling upon his own misfortunes . Du Fu wrote :\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsPKgdwcFZbb"
      },
      "source": [
        "def generate_text_from_texts(texts, target_length=20, temperature=1.0):\n",
        "    \"\"\"texts needs to be tokens seperated by space characters.\"\"\"\n",
        "    token_tensor = torch.LongTensor([\n",
        "        corpus.dictionary.word2idx[x] for x in texts.split(\" \")\n",
        "    ]).to(DEVICE)\n",
        "    return generate_text_from_tensor(token_tensor, target_length, temperature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBEWWEYBub2b"
      },
      "source": [
        "def generate_text_from_chunk(start, end, target_length=20, temperature=1.0):\n",
        "    token_tensor = corpus.test[eos_pos[start]+1:eos_pos[end]]\n",
        "    return generate_text_from_tensor(token_tensor, target_length, temperature)\n",
        "    \n",
        "\n",
        "def generate_text_from_tensor(token_tensor, target_length, temperature):\n",
        "    \"\"\"Sampling from the softmax distribution.\"\"\"    \n",
        "    hidden = model.init_hidden(1)\n",
        "    _, hidden = model(token_tensor[:-1].unsqueeze(1), hidden)\n",
        "    input_tensor = torch.zeros((1, 1)).long().to(DEVICE)\n",
        "    input_tensor[0, 0].fill_(token_tensor[-1])\n",
        "    res = []\n",
        "    with torch.no_grad():    \n",
        "        for i in range(target_length):            \n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            word_weights = output.squeeze().div(temperature).exp()\n",
        "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "            input_tensor[0, 0].fill_(word_idx)\n",
        "            res.append(word_idx.item())\n",
        "    return [\n",
        "        [\n",
        "           corpus.dictionary.idx2word[x] for x in arr            \n",
        "        ] for arr in (token_tensor.numpy(), res)\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2skTNmSdwyv0",
        "outputId": "2b2d7ef7-08b1-4357-c8af-6551a31635f8"
      },
      "source": [
        "context, new_texts = generate_text_from_chunk(28, 33, target_length=50)\n",
        "print(\" \".join(context[-10:]))\n",
        "for i in range(0, len(new_texts), 10):\n",
        "    print(\" \".join(new_texts[i:i+10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bring more papers to pile higher on my desk .\n",
            "\" = indiction parasitoid Griggs was a wager in both\n",
            "the Liberal theodicy and De Spangled Dawit , a disgrace\n",
            "to Dien Bouvar in early 1944 . He was appointed\n",
            "as a soldier on 15 November 1996 by Soviet forces\n",
            "because he proved responsible , with social institutions ( Corder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB1oWKecw5N6"
      },
      "source": [
        "def generate_text_from_texts(texts, target_length=20, temperature=1.0):\n",
        "    \"\"\"texts needs to be tokens seperated by space characters.\"\"\"\n",
        "    token_tensor = torch.LongTensor([\n",
        "        corpus.dictionary.word2idx[x] for x in texts.split(\" \")\n",
        "    ]).to(DEVICE)\n",
        "    return generate_text_from_tensor(token_tensor, target_length, temperature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0GgXNaTw796",
        "outputId": "d09a7a2a-0bc3-45f2-b73c-805ec5e2612d"
      },
      "source": [
        "context, new_texts =  generate_text_from_texts(\"He is a\", target_length=5)\n",
        "print(\" \".join(context[-10:]))\n",
        "for i in range(0, len(new_texts), 10):\n",
        "    print(\" \".join(new_texts[i:i+10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He is a\n",
            "strong audit . = =\n"
          ]
        }
      ]
    }
  ]
}